{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdglvlcfzbK8"
   },
   "source": [
    "#Reference Solution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gW4_hSzIaNQ",
    "outputId": "25b29601-d595-46fc-f257-9398d959d597",
    "ExecuteTime": {
     "end_time": "2025-05-02T09:06:22.224789Z",
     "start_time": "2025-05-02T09:06:22.195385Z"
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/TRACO/bin/python\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(sys.executable)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgoogle\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcolab\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[32m      5\u001B[39m drive.mount(\u001B[33m'\u001B[39m\u001B[33m/content/drive\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'google'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D91jVvT4OWj7"
   },
   "source": [
    "## Ab hier Loss function(dice loss)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BNsg4ZeUe8k5",
    "ExecuteTime": {
     "end_time": "2025-05-02T09:03:22.966986744Z",
     "start_time": "2025-05-02T09:03:16.623847Z"
    }
   },
   "source": [
    "class loss():\n",
    "  #\n",
    "  def dice_loss(pred, target, smooth = 1.):\n",
    "      pred = pred.contiguous()\n",
    "      target = target.contiguous()\n",
    "\n",
    "      intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "\n",
    "      loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "\n",
    "      return loss.mean()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN-woBEkOOnL"
   },
   "source": [
    "## Ab hier Model Aufbau"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G1FtmjSOaKmy",
    "ExecuteTime": {
     "end_time": "2025-05-02T09:03:29.563159Z",
     "start_time": "2025-05-02T09:03:29.285035Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import math\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.9):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "\n",
    "class ResNetUNet(torch.nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        #self.hidden_size1 = 64\n",
    "        #self.hidden_size2\n",
    "        #self.hidden_size3\n",
    "        #self.hidden_size4\n",
    "\n",
    "        self.layer0 = torch.nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = torch.nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = torch.nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)#skip connections\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DiagnosticOptions' from 'torch.onnx._internal.exporter' (/home/tim/anaconda3/envs/TRACO/lib/python3.12/site-packages/torch/onnx/_internal/exporter/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset, DataLoader\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m transforms, datasets, models\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m resnet18, ResNet18_Weights\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmath\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torchvision/__init__.py:6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodulefinder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Module\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torchvision/models/__init__.py:2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01malexnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconvnext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdensenet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mefficientnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torchvision/models/convnext.py:8\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmisc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstochastic_depth\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtransforms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_presets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torchvision/ops/__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_register_onnx_ops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _register_custom_op\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mboxes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      3\u001B[39m     batched_nms,\n\u001B[32m      4\u001B[39m     box_area,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     remove_small_boxes,\n\u001B[32m     14\u001B[39m )\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mciou_loss\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m complete_box_iou_loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torchvision/ops/_register_onnx_ops.py:5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwarnings\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m symbolic_opset11 \u001B[38;5;28;01mas\u001B[39;00m opset11\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msymbolic_helper\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m parse_args\n\u001B[32m      8\u001B[39m _ONNX_OPSET_VERSION_11 = \u001B[32m11\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/TRACO/lib/python3.12/site-packages/torch/onnx/__init__.py:46\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01merrors\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CheckerError  \u001B[38;5;66;03m# Backwards compatibility\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     35\u001B[39m     _optimize_graph,\n\u001B[32m     36\u001B[39m     _run_symbolic_function,\n\u001B[32m   (...)\u001B[39m\u001B[32m     43\u001B[39m     unregister_custom_op_symbolic,\n\u001B[32m     44\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexporter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001B[39;00m\n\u001B[32m     47\u001B[39m     DiagnosticOptions,\n\u001B[32m     48\u001B[39m     ExportOptions,\n\u001B[32m     49\u001B[39m     ONNXProgram,\n\u001B[32m     50\u001B[39m     ONNXProgramSerializer,\n\u001B[32m     51\u001B[39m     ONNXRuntimeOptions,\n\u001B[32m     52\u001B[39m     InvalidExportOptionsError,\n\u001B[32m     53\u001B[39m     OnnxExporterError,\n\u001B[32m     54\u001B[39m     OnnxRegistry,\n\u001B[32m     55\u001B[39m     dynamo_export,\n\u001B[32m     56\u001B[39m     enable_fake_mode,\n\u001B[32m     57\u001B[39m )\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnxruntime\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     60\u001B[39m     is_onnxrt_backend_supported,\n\u001B[32m     61\u001B[39m     OrtBackend \u001B[38;5;28;01mas\u001B[39;00m _OrtBackend,\n\u001B[32m     62\u001B[39m     OrtBackendOptions \u001B[38;5;28;01mas\u001B[39;00m _OrtBackendOptions,\n\u001B[32m     63\u001B[39m     OrtExecutionProvider \u001B[38;5;28;01mas\u001B[39;00m _OrtExecutionProvider,\n\u001B[32m     64\u001B[39m )\n\u001B[32m     66\u001B[39m __all__ = [\n\u001B[32m     67\u001B[39m     \u001B[38;5;66;03m# Modules\u001B[39;00m\n\u001B[32m     68\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msymbolic_helper\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    114\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mis_onnxrt_backend_supported\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    115\u001B[39m ]\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'DiagnosticOptions' from 'torch.onnx._internal.exporter' (/home/tim/anaconda3/envs/TRACO/lib/python3.12/site-packages/torch/onnx/_internal/exporter/__init__.py)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-Pt9I2qqbNB",
    "outputId": "7883f5bf-0241-4bee-a503-5fdcdb53fea8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5h1TJJAnaX2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load('reference-model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j-_vlTv3B90"
   },
   "source": [
    "### Video einlesen und model testen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EcpONygL3HUV",
    "outputId": "bc31b0ff-fd77-4296-cb22-5b46e96b639d",
    "ExecuteTime": {
     "end_time": "2025-05-02T09:03:22.984311209Z",
     "start_time": "2025-05-02T09:02:07.897236Z"
    }
   },
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "cap = cv2.VideoCapture(\"leaderboard_data/test001.mp4.mp4\")\n",
    "frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fc = 0\n",
    "ret = True\n",
    "prediction = []\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "frameWidth,frameHeight,channels =0,0,0\n",
    "while (fc < frameCount and ret):\n",
    "  #take frame\n",
    "  ret, buf = cap.read()\n",
    "  # convert to RGB\n",
    "  im_rgb = cv2.cvtColor(buf, cv2.COLOR_BGR2RGB)\n",
    "  #resize frame\n",
    "  im_rgb = cv2.resize(im_rgb, dsize=(256,256))\n",
    "\n",
    "  #get frame size\n",
    "  frameWidth1,frameHeight1,channels1 = buf.shape\n",
    "  frameWidth,frameHeight,channels = frameWidth1,frameHeight1,channels1\n",
    "\n",
    "  #bild ans model geben\n",
    "  im_rgb = transform(im_rgb)\n",
    "  img = torch.unsqueeze(im_rgb,0).to(device)\n",
    "  some_result = model(img)\n",
    "  some_result = torch.squeeze(some_result,0)\n",
    "  some_result = torch.squeeze(some_result,0)\n",
    "  some_result = some_result.cpu().detach().numpy()\n",
    "  plt.imshow(some_result, interpolation='nearest')\n",
    "  plt.show()\n",
    "  #hier speichern\n",
    "  prediction.append(some_result)\n",
    "  fc += 1\n",
    "print(fc)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(python:24657): GStreamer-CRITICAL **: 11:02:11.982: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed\n",
      "[ WARN:0@2.905] global cap_gstreamer.cpp:1436 open OpenCV | GStreamer warning: Error opening bin: no element \"leaderboard_data\"\n",
      "[ WARN:0@2.905] global cap_gstreamer.cpp:1173 isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m ret = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     12\u001B[39m prediction = []\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m transform = transforms.Compose([transforms.ToTensor()])\n\u001B[32m     14\u001B[39m frameWidth,frameHeight,channels =\u001B[32m0\u001B[39m,\u001B[32m0\u001B[39m,\u001B[32m0\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m (fc < frameCount \u001B[38;5;129;01mand\u001B[39;00m ret):\n\u001B[32m     16\u001B[39m   \u001B[38;5;66;03m#take frame\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'transforms' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj3qdrHEcuOE"
   },
   "source": [
    "# Ab hier das max instensity herrausfinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLa6dpNKaYgh"
   },
   "outputs": [],
   "source": [
    "def get_densest_numpy_patches(image):\n",
    "  radius = 10\n",
    "  maximum_value_list = []\n",
    "  #get maximum value coordinates\n",
    "  a = np.argmax(image)\n",
    "  l=image.shape[0]\n",
    "  c = a%l\n",
    "  r = int(a/l)\n",
    "  #make threshold the first highest value that could be found\n",
    "  threshold = image[r,c]/2\n",
    "  #print((r*l+c)==a)\n",
    "  k=0\n",
    "  #for every pixel that is at least as intens as the first\n",
    "  #Annahme: Alle Köpfe haben am Ende wenigstens einen Pixel mit der höchsten intensität. Ansonsten einfach eine Range von 5 % einrichten threshold * 0.95\n",
    "  #while image[r,c] >= threshold:\n",
    "  NUMBER_OF_HEX = 1\n",
    "  for i in range(NUMBER_OF_HEX):\n",
    "    if( r < 5 or c < 5 ):\n",
    "      passes = False\n",
    "      while(passes == False):\n",
    "        image = cv2.circle(image, (c,r), 2,0, -1)\n",
    "        a = np.argmax(image)\n",
    "        l=image.shape[0]\n",
    "        c = a%l\n",
    "        r = int(a/l)\n",
    "        if(r <5 or c<5):\n",
    "          passes = True\n",
    "    k+=1\n",
    "    #print(r,c)\n",
    "    i=r\n",
    "    j=c\n",
    "    #Alle pixel darum auf 0 setzten damit argmax neuen höchsten finden kann\n",
    "    image = cv2.circle(image, (c,r), radius,0, -1)\n",
    "    #f1 = plt.figure(k)\n",
    "    #plt.imshow(image, interpolation='nearest')\n",
    "    #Höhepunkt hinzufügen\n",
    "    maximum_value_list.append([c,r])\n",
    "    a = np.argmax(image)\n",
    "    c = a%256\n",
    "    r = int(a/256)\n",
    "  return maximum_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ktUEPRW5Tts"
   },
   "outputs": [],
   "source": [
    "###hier wird weitergemacht und die peaks gesucht\n",
    "list_max_value_unordered = []\n",
    "for i in prediction:\n",
    "  list = get_densest_numpy_patches(i)\n",
    "  list_max_value_unordered.append(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9jT_1ef5_yI",
    "outputId": "0e1ec915-e3f1-4830-8261-bae6a5ecb5cc"
   },
   "outputs": [],
   "source": [
    "print(len(list_max_value_unordered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwMlxAYObXuD",
    "outputId": "329a1b48-454a-45ec-ffb1-4a9e3b0fd408"
   },
   "outputs": [],
   "source": [
    "print(list_max_value_unordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjgZTBrkvfC7"
   },
   "source": [
    "## Ab hier list erstellen mit Positions in richtiger reihenfolge der IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq1s5-BYkpRK"
   },
   "source": [
    "Angewendet auf somedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxWajdNlvfyN"
   },
   "outputs": [],
   "source": [
    "list_max_value_ordered = []\n",
    "#ordered list is first row\n",
    "list_max_value_ordered = [list_max_value_unordered[0]]\n",
    "def assign_ID():\n",
    "  #Die länge der positionen ist die länge des ersten prediction\n",
    "  pos_sum = len(list_max_value_unordered[0])\n",
    "  global list_max_value_ordered\n",
    "  #über alle prediction iterieren, ersten weglassen\n",
    "  for i in range(1,np.shape(list_max_value_unordered)[0]):\n",
    "    #get coordinates\n",
    "    for l in range(pos_sum):\n",
    "      pos1 = list_max_value_ordered[i-1]\n",
    "      pos2 = list_max_value_unordered[i]\n",
    "    # neuen eintrag machen in der länge der anzahl der hexbug\n",
    "    pp=[]\n",
    "    for o in range(pos_sum):\n",
    "      pp.append([0,0])\n",
    "    list_max_value_ordered.append(pp)\n",
    "\n",
    "    #calculate shortest distance\n",
    "    for j in range(pos_sum):\n",
    "      d_shortest =10000\n",
    "      index = 0\n",
    "      for k in range(len(pos2)):\n",
    "        #minimale strecke berechnen\n",
    "        d = np.sqrt((pos1[j][0]-pos2[k][0])**2 + (pos1[j][1]-pos1[k][1])**2)\n",
    "        #und wenn strecke kleiner dann indices abspeichern\n",
    "        if(d<d_shortest):\n",
    "          d_shortest = d\n",
    "          index = k\n",
    "      list_max_value_ordered[i][j][0] = pos2[index][0]\n",
    "      list_max_value_ordered[i][j][1] = pos2[index][1]\n",
    "      #pos entfernen\n",
    "      del pos2[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7-L9F4rokZX",
    "outputId": "de3afed8-1327-40a0-b58f-0242073e4269"
   },
   "outputs": [],
   "source": [
    "print(len(list_max_value_unordered))\n",
    "print(list_max_value_unordered)\n",
    "assign_ID()\n",
    "print(list_max_value_ordered)\n",
    "print(len(list_max_value_ordered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Pjau5hlkd7"
   },
   "source": [
    "## Positionen in Json speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puK-D5h5lpN1",
    "outputId": "976ca2df-c6ad-4b08-fa34-d82399bfee5b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(list_max_value_ordered)\n",
    "json_predicted = {}\n",
    "json_predicted['rois'] = []\n",
    "frame=0\n",
    "for i in list_max_value_ordered:\n",
    "  id=0\n",
    "  for j in i:\n",
    "    json_predicted['rois'].append({\n",
    "    'z' : frame,\n",
    "    'id' : id,\n",
    "    'pos': j\n",
    "    })\n",
    "    id+=1\n",
    "  frame +=1\n",
    "print(json_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcWDhxmKCVRb",
    "outputId": "10120a01-2a0d-4c37-d3d0-4dbacb13c7b7"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open a CSV file in write mode\n",
    "with open('predicted_data.csv', mode='w', newline='') as csv_file:\n",
    "    # Define the fieldnames for the CSV file\n",
    "    fieldnames = ['','t', 'hexbug', 'x', 'y']\n",
    "\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(fieldnames)\n",
    "    idx = 0\n",
    "    # Write data from the list to the CSV file\n",
    "    for t, frame in enumerate(list_max_value_ordered):\n",
    "        for hexbug, pos in enumerate(frame):\n",
    "            pos_x = np.round((pos[0]/256)*frameHeight,2)\n",
    "            pos_y = np.round((pos[1]/256)*frameWidth,2)\n",
    "            x, y = pos_x, pos_y\n",
    "            writer.writerow([idx,t, hexbug, x, y])\n",
    "            idx += 1\n",
    "\n",
    "print(\"CSV file has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
