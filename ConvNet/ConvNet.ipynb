{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2025-05-16T12:27:53.174916Z",
     "start_time": "2025-05-16T12:27:53.146621Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Zelle 2: Beispiel CNN-Modell\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_bugs=1): # Add num_bugs as an argument\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_bugs * 2)  # Output coordinates for multiple bugs\n",
    "        )\n",
    "        self.num_bugs = num_bugs # Store num_bugs\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(self.conv(x))\n",
    "        return output.reshape(-1, self.num_bugs, 2) # Reshape to (batch_size, num_bugs, 2)\n",
    "\n",
    "\n",
    "model = ConvNet()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIpvwe5eR25S",
    "outputId": "002d2b4d-c756-4392-bad2-4f455d1d9f8e",
    "ExecuteTime": {
     "end_time": "2025-05-16T12:27:56.177325Z",
     "start_time": "2025-05-16T12:27:56.175563Z"
    }
   },
   "id": "WIpvwe5eR25S",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "d07255e3ebf8e6e2",
    "ExecuteTime": {
     "end_time": "2025-05-16T13:16:28.404365Z",
     "start_time": "2025-05-16T13:16:28.396668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class VideoTrackingDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir, limit, resize=(512, 512), transform=None, max_objects=10):\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize(resize),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.max_objects = max_objects\n",
    "\n",
    "        self.samples = self._gather_samples()\n",
    "        if limit is not None:\n",
    "            self.samples = self.samples[:limit]\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        samples = []\n",
    "        for file in os.listdir(self.video_dir):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                base = file[:-4]\n",
    "                video_path = os.path.join(self.video_dir, file)\n",
    "                label_path = os.path.join(self.label_dir, f\"{base}.csv\")\n",
    "                if os.path.exists(label_path):\n",
    "                    samples.append((video_path, label_path))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([self._count_frames(v) for v, _ in self.samples])\n",
    "\n",
    "    def _count_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        return count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cumulative = 0\n",
    "        for video_path, label_path in self.samples:\n",
    "            num_frames = self._count_frames(video_path)\n",
    "            if index < cumulative + num_frames:\n",
    "                frame_idx = index - cumulative\n",
    "                return self._get_sample(video_path, label_path, frame_idx)\n",
    "            cumulative += num_frames\n",
    "        raise IndexError(\"Index außerhalb des Datasets\")\n",
    "\n",
    "    def _get_sample(self, video_path, label_path, frame_idx):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret:\n",
    "            raise ValueError(f\"Frame {frame_idx} konnte nicht gelesen werden.\")\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame)\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        df = pd.read_csv(label_path)\n",
    "        frame_data = df[df['t'] == frame_idx]\n",
    "\n",
    "        coords = frame_data[['x', 'y']].values\n",
    "        coords = coords / np.array([original_width, original_height])\n",
    "        coords = coords.astype(np.float32)\n",
    "\n",
    "        # Padding\n",
    "        padded = np.full((self.max_objects, 2), -1, dtype=np.float32)\n",
    "        num_coords = min(len(coords), self.max_objects)\n",
    "        padded[:num_coords] = coords[:num_coords]\n",
    "\n",
    "        image = self.transform(image)\n",
    "        coords_tensor = torch.tensor(padded, dtype=torch.float32)\n",
    "\n",
    "        return image, coords_tensor\n"
   ],
   "id": "d07255e3ebf8e6e2",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "id": "561c914ed259bca7",
    "ExecuteTime": {
     "end_time": "2025-05-16T13:17:03.399805Z",
     "start_time": "2025-05-16T13:17:03.393111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = VideoTrackingDataset(\"../training\", \"../training\", limit=5, max_objects=10)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False)"
   ],
   "id": "561c914ed259bca7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "id": "ba0ec4ab3fe6161c",
    "outputId": "060ec786-80a3-4f11-a7bf-dbede16e47a3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-05-16T13:17:17.631568Z",
     "start_time": "2025-05-16T13:17:07.847877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "\n",
    "# Zelle 4: Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ],
   "id": "ba0ec4ab3fe6161c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\tdann\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5, 10, 2])) that is different to the input size (torch.Size([5, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m criterion = nn.MSELoss()\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    629\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    630\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m631\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    633\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    634\u001B[39m         \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    635\u001B[39m         \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    673\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    674\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m675\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    676\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    677\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     49\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m         data = \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     53\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\traco\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m     49\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     53\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mVideoTrackingDataset.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m index < cumulative + num_frames:\n\u001B[32m     50\u001B[39m         frame_idx = index - cumulative\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     52\u001B[39m     cumulative += num_frames\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mIndex außerhalb des Datasets\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 56\u001B[39m, in \u001B[36mVideoTrackingDataset._get_sample\u001B[39m\u001B[34m(self, video_path, label_path, frame_idx)\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_sample\u001B[39m(\u001B[38;5;28mself\u001B[39m, video_path, label_path, frame_idx):\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m     cap = \u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mVideoCapture\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n\u001B[32m     58\u001B[39m     ret, frame = cap.read()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "id": "6eac08873320ff26",
    "ExecuteTime": {
     "end_time": "2025-05-16T12:21:58.877472Z",
     "start_time": "2025-05-16T12:21:58.877472Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"conv_net.pth\")",
   "id": "6eac08873320ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Lade das trainierte Modell\n",
    "model = torch.load(\"conv_net.pth\", weights_only=False)\n",
    "model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "\n",
    "# Definiere die Transformationen für die Eingabebilder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Pfad zum Video\n",
    "video_path = \"../training/training02.mp4\"\n",
    "\n",
    "# Lade das Video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Erstelle eine leere Liste, um die Vorhersagen zu speichern\n",
    "predictions = []\n",
    "\n",
    "# Verarbeite jeden Frame des Videos\n",
    "frame_count = 0\n",
    "row_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Konvertiere den Frame in ein PIL-Bild\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame)\n",
    "\n",
    "    # Wende die Transformationen auf das Bild an\n",
    "    image = transform(image)\n",
    "\n",
    "    # Füge eine Batch-Dimension hinzu\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Führe die Vorhersage mit dem Modell aus\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Extrahiere die x- und y-Koordinaten aus der Vorhersage\n",
    "    x, y = output[0, 0].tolist()\n",
    "\n",
    "    # Hole die Originalgröße des Bildes\n",
    "    original_height, original_width, _ = frame.shape\n",
    "\n",
    "    # Rechne die x- und y-Koordinaten zurück\n",
    "    x_original = x * original_width\n",
    "    y_original = y * original_height\n",
    "\n",
    "    # Füge die Vorhersage zur Liste hinzu\n",
    "    predictions.append([row_count, frame_count, 0, x_original, y_original])\n",
    "\n",
    "    frame_count += 1\n",
    "    row_count += 1\n",
    "\n",
    "# Schließe das Video\n",
    "cap.release()\n",
    "\n",
    "# Erstelle einen Pandas DataFrame aus den Vorhersagen\n",
    "df = pd.DataFrame(predictions, columns=[\"\", \"t\", \"hexbug\", \"x\", \"y\"])\n",
    "\n",
    "# Speichere den DataFrame in einer CSV-Datei\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "print(\"Vorhersagen wurden in predictions.csv gespeichert.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKt_kZsPCfX3",
    "outputId": "e14326ac-200c-4b5b-de82-dc915d463ea0"
   },
   "id": "hKt_kZsPCfX3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
